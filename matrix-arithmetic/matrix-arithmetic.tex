\documentclass[17pt]{amsart}

\usepackage{fullpage,extsizes}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\lra}{\longrightarrow}
\DeclareMathOperator{\rref}{rref}

\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}

\begin{document}
\title{The connection between row reduction and matrix arithmetic}

% Indexing matrices

% We write $A_{i,j}$ for the $(i,j)$-entry of $A$, i.e., the number in row $i$ and column $j$. 

% Matrix addition and subtraction

% You can add and subtract matrices $A$ and $B$ if they have the same size\footnote{have the same number of rows and the same number of columns}.
% Addition and subtraction are computed \emph{entry-by-entry}
% \[
% (A\pm B)_{i,j} = A_{i,j} \pm B_{i,j}.
% \]
% A \emph{zero matrix} is a matrix all of whose entries are $0$. For all $m$ and $n$, there is a unique $m\times n$ zero matrix. We write $\bzero$ for this matrix, or $\bzero^{m\times n}$ if the size isn't clear from context.

% Addition and subtraction of matrices satisfy the same properties as addition and multiplication of numbers.

% Multiplying matrices by scalars

% You multiply a matrix $A$ by a scalar $s$ by multiplying every entry of $A$ by $s$.
% \[
% (sA)_{i,j} = sA_{i,j}.
% \]
% Scalar multiplication satisfies the usual rules of multiplication. Note that there are \emph{two} distributive laws involving scalar multiplication:
% \[
% s(A+B)=sA+sB,\qquad (s+t)A = sA + tA.
% \]

% Matrix multiplication

% You can multiply an $n$-dimensional row vector by an $n$-dimensional column vector as follows:
% \[
% \begin{pmatrix}
% a_1&a_2&\cdots &a_n
% \end{pmatrix}
% \begin{pmatrix}
% b_1\\b_2\\\vdots\\b_n
% \end{pmatrix}
% =a_1b_1+a_2b_2+\cdots a_nb_n.
% \]
% The order of the factors -- row vector on the left, column vector on the right -- is important.

% The matrix product $AB$ is defined whenever the number of columns of $A$ equals the number of rows of $B$: If $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix, then $AB$ is defined and
% \begin{align*}
% (AB)_{i,j}&=\text{($i$-th row of $A$)($j$-th column of $B$)} \\
% &= \begin{pmatrix}
% A_{i,1}&A_{i,2}&\cdots&A_{i,n}
% \end{pmatrix}
% \begin{pmatrix}
% B_{1,j}\\B_{2,j}\\\vdots\\B_{n,j}
% \end{pmatrix}.
% \end{align*}

% Matrix multiplication satisfies the same properties as multiplication of numbers \emph{except for the commutative law}:
% \[
% AB\neq BA,\quad \text{in general.}
% \]

% Note that the products $AB$ and $BA$ are both defined if and only if $A$ and $B$ are square matrices of the same size.

% The identity matrix

% For a given $n$, let $I_n$ be the matrix with 1s along the main diagonal\footnote{the diagonal spanning the matrix from top-left to bottom-right} and 0s everywhere else:
% \[
% (I_n)_{i,j} = \begin{cases}
% 1&\text{if $i=j$,}\\
% 0&\text{if $i\neq j$.}
% \end{cases}
% \]
% For example,
% \[
% I_1 = \begin{pmatrix}1\end{pmatrix},\quad
% I_2 = \begin{pmatrix}1&0\\0&1\end{pmatrix},\quad
% I_3 = \begin{pmatrix}
% 1&0&0\\0&1&0\\0&0&1
% \end{pmatrix},\quad
% I_4 = \begin{pmatrix}
% 1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&1
% \end{pmatrix},\quad\ldots.
% \]

% The identity matrix is to matrix multiplicaton as $1$ is to multiplication of numbers: If $A$ is an $m\times n$ matrix, then
% \[
% I_mA = A = AI_n.
% \]
% When the size of the identity matrix can be inferred from context, we write $I$ instead of $I_n$.

% Matrix inversion


% A matrix is \emph{invertible} if it has a multiplicative inverse, i.e., if there is a matrix $A^{-1}$ such that
% \[
% A^{-1}A = I = AA^{-1}
% \]
% If $A$ is invertible, its inverse is unique.

% Only square matrices can be invertible, and $A^{-1}$, if it exists, has the same size as $A$.

% Let $A$ and $B$ be square matrices of the same size. Then
% \[
% AB=I\quad\text{if and only if} \quad
% B=A^{-1}\quad\text{if and only if}\quad
% BA=I.
% \]
% Thus, to verify that $B$ is the inverse of $A$, you need only check that \emph{one} of the products $AB$ and $BA$ is $I$.

% Matrix transposition

% We say that vectors
% $$\begin{pmatrix}a_1&a_2&\cdots&a_n\end{pmatrix}
% \qquad\text{and}\qquad
% \begin{pmatrix}a_1\\a_2\\\vdots\\a_n\end{pmatrix}$$
% are \emph{transposes} of each other. We write $\ba^T$ for the transpose of a vector $\ba$. If $\ba$ is a row vector, then $\ba^T$ is a column vector, and vice-versa:
% \[
% \begin{pmatrix}a_1&a_2&\cdots&a_n\end{pmatrix}^T=
% \begin{pmatrix}a_1\\a_2\\\vdots\\a_n\end{pmatrix}
% \qquad\text{and}\qquad
% \begin{pmatrix}a_1\\a_2\\\vdots\\a_n\end{pmatrix}^T=
% \begin{pmatrix}a_1&a_2&\cdots&a_n\end{pmatrix}.
% \]
% If $A$ is a matrix, then its transpose $A^T$ is the matrix whose $i$-th row is the transpose of the $i$-th column of $A$. Equivalently, $A^T$ is the matrix whose $(i,j)$-entry is the $(j,i)$-entry of $A$:
% \[
% (A^T)_{i,j} = A_{j,i}.
% \]
% For example:
% \[
% \begin{pmatrix}
% 1&2\\3&4
% \end{pmatrix}^T=
% \begin{pmatrix}
% 1&3\\2&4
% \end{pmatrix},\quad
% \begin{pmatrix}
% 1&2&3\\4&5&6\\7&8&9
% \end{pmatrix}^T=
% \begin{pmatrix}
% 1&4&7\\2&5&8\\3&6&9
% \end{pmatrix},\quad
% \begin{pmatrix}
% 1&2&3&4\\5&6&6&8\\9&10&11&12\\13&14&15&16
% \end{pmatrix}^T=
% \begin{pmatrix}
% 1&5&9&13\\
% 2&6&10&14\\
% 3&7&11&15\\
% 4&8&12&16
% \end{pmatrix}
% \]

\section{Find $U$ such that $UA=\rref(A)$}
\textbf{Notation:} If $B$ is the matrix you get by applying the elementary row operation $O$ to $A$, we write
\[
B=OA.
\]
\textbf{To find $U$:} Row-reduce the \emph{horizontal join}
\[
\left(
\begin{array}{c|c}
A&I
\end{array}
\right),
\]
i.e., apply EROs $O_1,O_2,\ldots,O_n$ to 
$\left(
\begin{array}{c|c}
A&I
\end{array}
\right)$
until it has the form $\left(
\begin{array}{c|c}
\rref A&U
\end{array}
\right)$:
\begin{multline*}
\left(
\begin{array}{c|c}
A&I
\end{array}
\right) \overset{O_1}\lra 
\left(
\begin{array}{c|c}
O_1A&O_1I
\end{array}
\right)\overset{O_2}\lra
\left(
\begin{array}{c|c}
O_2O_1A&O_2O_1I
\end{array}
\right)\overset{O_3}
\lra\\
\cdots
\overset{O_n}\lra
\left(
\begin{array}{c|c}
\rref A&U
\end{array}
\right).
\end{multline*}
Then:
\[
\text{$U$ is invertible and $UA=\rref A$.}
\]

\section{Finding the inverse of a matrix}
Finding the inverse of an invertible matrix is a \emph{special case} of the problem of finding an invertible $U$ such that $UA=\rref A$:
\[
\text{\textbf{If} $\rref A = I$, \textbf{then}
$UA=I$.}
\]
Since $U$ is invertible, so is $A$ (why?) and
\[
A^{-1} = U.
\]
In particular:
\[
\text{\textbf{If} $\rref A=I$ \textbf{then} $A$ is invertible.}
\]
The \emph{converse} of this implication is also true:
\[
\text{\textbf{If} $A$ is invertible, \textbf{then} $\rref A=I$.}
\]
Thus:
\[
\text{$A$ is invertible \textbf{if and only if} $\rref A=I$.}
\]

\section{Why this works: Elementary matrices}
Let $E(O)$ be the matrix you get by applying the ERO $O$ to the identity matrix, $I$:
\[
E(O) = OI.
\]
$E(O)$ is called the elementary matrix (EM) of $O$. A matrix $E$ is an EM if $E=E(O)$ for some ERO $O$.

\bigskip\textbf{EM fact 1:} $E(O)A$ is the matrix you get by applying $O$ to $A$.
\[
E(O)A=OA.
\]

As above, suppose
\[
\left(
\begin{array}{c|c}
\rref A&U
\end{array}
\right)=O_n\cdots O_2O_1\left(
\begin{array}{c|c}
A&I
\end{array}
\right).
\]
Set
\[
E_i = E(O_i).
\]
Since\footnote{The first identity describes how EROs interact with horizontal joins of matrices. The second follows from the \text{EM fact 1}, relating EROs and EMs.}
\begin{align*}
O_n\cdots O_2O_1\left(
\begin{array}{c|c}
A&I
\end{array}
\right)&=
\left(
\begin{array}{c|c}
O_n\cdots O_2O_1A&O_n\cdots O_2O_1I
\end{array}
\right),\\
&= \left(\begin{array}{c|c}
E_n\cdots E_2E_1A&
E_n\cdots E_2E_1
\end{array}\right),
\end{align*}
we have:
\[
U=E_n\cdots E_2E_1\quad\text{and}\quad UA=\rref A.
\]

\section{Writing a matrix as a product of elementary matrices}
Suppose that $A$ is invertible.
Then, as we observed above, $U$ is invertible and
\[
A^{-1}=U.
\]
Inverting both sides of this identity yields
\[
A=U^{-1}
\]
Since $U=E_n\cdots E_2E_1$,
\[
U^{-1}=E_1^{-1}E_2^{-1}\cdots E_n^{-1}.
\]
Remember: the inverse of a product is a product of inverses, \emph{in the reversed order}.
Thus, $A$ is a product of \emph{inverses of} elementary matrices:
\[
A = U^{-1}=E_1^{-1}E_2^{-1}\cdots E_n^{-1}.
\]
But:

\bigskip\textbf{EM fact 2:} Inverses of EMs exist and are, themselves, EMs. More precisely, the inverse of the EM of an ERO is the EM of the inverse ERO:
\[
E(O)^{-1} = E(O^{-1})
\]

Perhaps we have not stated the following explicitly:

\bigskip\textbf{ERO fact}: Every ERO is invertible.

\begin{center}
\begin{tabular}{c|c|c}
Type & $O$ & $O^{-1}$\\ \hline
1& swap rows $i$ and $j$ & swap rows $i$ and $j$\\
2& multiply row $i$ by $c$ ($c\neq 0$) & multiply row $i$ by $1/c$\\
3& add $k\times$(row $i$) to row $j$& add $-k\times$(row $i$) to row $j$
\end{tabular}
\end{center}

Thus, 
\[
A=E_1^{-1}E_2^{-1}\cdots E_n^{-1}
\]
is an expression for $A$ as a product of elementary matrices.
\end{document}